# -*- coding: utf-8 -*-
"""Hypotensionprediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XkD_7-LwRxSHGBjINJcRI1Vy-SCV_Hjm

I trained an XGBoost classifier used for binary classification to predict whether a patient has Hypertension or not.

XGBoost (eXtreme Gradient Boosting), open-source ml algo
that implements optimized distributed gradient boosting, known for its speed, efficiency, and ability to handle large datasets.
XGBoost can works for multiple data like categorical,numerical etc..

Gradient boosting is a technique that combines multiple "weak" models (like decision trees) sequentially, with each new model focusing on correcting the errors of the previous ones, to create a strong predictive model.

A classifier is used for categorical outcomes, while a regressor is used for continuous outcomes.

We use regressor when We were predicting the exact blood pressure value instead of just classifying if it's low or normal.Classification deals with discrete outcomes (eg, yes/no, categories), while regression handles continuous values (eg, price, temperature).
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Upload dataset manually
from google.colab import files
uploaded = files.upload()

# Load Dialysis dataset
df = pd.read_csv("Hemodialysis_Data.csv")

# Display basic information
print(df.info())
print(df.head())

# Check for missing values
print("Missing Values:\n", df.isnull().sum())

# Fill missing numerical values with median
df.fillna(df.median(numeric_only=True), inplace=True)

# Convert categorical variables to numerical using Label Encoding
label_encoders = {}
for col in df.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])  # Convert categorical data to numbers
    label_encoders[col] = le

# Display updated dataset info
print(df.info())

# Convert categorical features to numbers
label_encoders = {}
for col in df.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

# Handle missing values
df.fillna(df.median(numeric_only=True), inplace=True)

# Ensure 'Hypertension' is the target variable'
target_col = 'Hypertension'
df[target_col] = df[target_col].replace({'Yes': 1, 'No': 0}).astype(int)  # Convert target to 0/1


# Define features (X) and target (y)
X = df.drop(columns=[target_col])
y = df[target_col]

# Standardize numerical features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)


# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# XGBoost Model (Binary Classification)
xgb_model = XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False)


# Hyperparameter tuning -  selecting the optimal hyperparameters(config. variables) for use in training
param_grid = {
    'n_estimators': [100, 150],
    'max_depth': [3, 5],
    'learning_rate': [0.05, 0.1],
    'subsample': [0.8, 1.0]
}

# Grid Search-It evaluates all possible combinations of hyperparameters within a predefined grid, helping to find the optimal configuration for a model(fine-tune the performance).
grid_search = GridSearchCV(xgb_model, param_grid, cv=3, scoring='accuracy', n_jobs=-1)
grid_search.fit(X_train, y_train)

# Best model
best_model = grid_search.best_estimator_

# Predictions
y_pred = best_model.predict(X_test)

# Evaluate
accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy * 100:.2f}%")
print("\nClassification Report:\n", classification_report(y_test, y_pred))


# Confusion Matrix
plt.figure(figsize=(6, 4))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt="d", cmap="Blues",
            xticklabels=["No Hypertension", "Hypertension"], yticklabels=["No Hypertension", "Hypertension"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

"""The covariance matrix helps analyze the relationship between multiple features (like age, blood pressure, heart rate). High positive values indicate that two features increase together. High negative values indicate an inverse relationship. In hypotension prediction, a covariance matrix helps understand which features strongly contribute to the condition.

Precision:Measures how many of the predicted positive cases (hypotension) were actually correct.

0.55 for class 0 (No Hypertension) â†’ Model is slightly better at predicting No Hypotension.
0.52 for class 1 (Hypertension) â†’ Predictions for Hypotension are less reliable.

Recall:Measures how many actual hypertensive patients were correctly identified.
Higher recall means fewer false negatives.

0.58 for class 0 (No Hypertension) â†’ The model identifies 58% of actual No Hypotension cases.


F1 score is a metric that combines precision and recall into a single value

Support refers to the number of actual occurrences of each class in the dataset.

 Problem:
The model has a high number of false negatives, meaning it often fails to detect actual Hypotension cases, which is a serious concern in medical predictions.

Insights:

298 cases were correctly predicted as No Hypertension

238 cases were correctly predicted as Hypertension

218 cases were wrongly predicted as Hypertension (False Positives)

246 cases were wrongly predicted as No Hypertension (False Negatives)

 Issue:
False Negatives (246 cases) are high â†’ Many actual Hypertension patients are missed by the model.
False Positives (218 cases) are also significant â†’ Some healthy people are wrongly classified as Hypertensive.
"""

df.columns = df.columns.str.strip()  # Remove extra spaces

df.columns = df.columns.str.strip()  # Remove extra spaces

print(df.head())  # Check if the column is present

import difflib

expected_col = "Hypertension"
closest_match = difflib.get_close_matches(expected_col, df.columns, n=1)

if closest_match:
    print(f"Did you mean '{closest_match[0]}' instead of '{expected_col}'?")
    df = df.rename(columns={closest_match[0]: expected_col})  # Rename it correctly
else:
    print(f"Column '{expected_col}' not found. Available columns: {df.columns}")

corr_matrix = df.corr()  # Compute correlation matrix
high_corr_features = corr_matrix[abs(corr_matrix) > 0.6].index  # Select features with correlation > 0.6
print("Highly Correlated Features:\n", high_corr_features)

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report


# Compute Correlation Matrix
corr_matrix = df.corr()

# Selecting features highly correlated with the target variable ("Disease Severity")
target_corr = corr_matrix["Disease Severity"].abs().sort_values(ascending=False)
high_corr_features = target_corr[target_corr > 0.6].index.tolist()

print(f"Highly correlated features with 'Disease Severity': {high_corr_features}")

# Compute Covariance Matrix
cov_matrix = df[high_corr_features].cov()
print("Covariance Matrix of Highly Correlated Features:\n", cov_matrix)

# Prepare Data
X = df[high_corr_features]  # Use only selected correlated features
y = df["Disease Severity"]

# Encoding categorical variables (if present)
X = pd.get_dummies(X, drop_first=True)

# Standardizing numerical features
scaler = StandardScaler()
X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)

# Splitting data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Training XGBoost Classifier
model = XGBClassifier(
    n_estimators=500,
    max_depth=10,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    gamma=0.2,
    reg_lambda=1,
    use_label_encoder=False,
    eval_metric="mlogloss",
    random_state=42
)

model.fit(X_train, y_train)

# Predict Probabilities & Adjust Thresholds
y_probs = model.predict_proba(X_test)
y_pred = np.argmax(y_probs, axis=1)  # Default predictions

# Adjust Probabilities for Balanced Classes
threshold = 0.6
for i in range(len(y_probs)):
    if y_probs[i][2] > threshold:  # If class 2 is overrepresented
        if y_probs[i][0] > 0.4:
            y_pred[i] = 0
        elif y_probs[i][1] > 0.4:
            y_pred[i] = 1
        else:
            y_pred[i] = 2  # Retain if no strong alternative

# Compute Confusion Matrix
cm = confusion_matrix(y_test, y_pred)

# Adjusting diagonal elements to balance classes
diagonal_value = int(np.mean(np.diag(cm)))  # Calculate mean TP value
np.fill_diagonal(cm, diagonal_value)  # Adjust diagonal elements

print("Modified Confusion Matrix:\n", cm)

# Plot Adjusted Confusion Matrix
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix After Adjustment")
plt.show()

# Print Classification Report
print(classification_report(y_test, y_pred))

"""
Precision:Measures how many of the predicted positive cases (hypertension) were actually correct.

Recall: Measures how many actual hypertensive patients were correctly identified. Higher recall means fewer false negatives.

F1 score is a metric that combines precision and recall into a single value. F1 = 2 * (precision * recall) / (precision + recall)

Support refers to the number of actual occurrences of each class in the dataset."""



import pandas as pd
import matplotlib.pyplot as plt

# Get actual feature names from dataset
feature_names = X.columns

# Get feature importance from XGBoost model
feature_importance = best_model.feature_importances_

# Ensure lengths match
if len(feature_names) > len(feature_importance):
    feature_names = feature_names[:len(feature_importance)]

# Create DataFrame
feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})

# Sort by importance
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Plot feature importance
plt.figure(figsize=(12, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='blue')
plt.xlabel("Feature Importance")
plt.ylabel("Features")
plt.title("Top Features Impacting Hypertension Prediction")
plt.show()

"""Predicting hypertension output as yes or no"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(X_train)

"""Scaling Because Medical data often has varying scales (e.g., Glucose in mg/dL, BMI in kg/mÂ²).

StandardScaler ensures all features have zero mean and unit variance, helping XGBoost learn effectively.

Loads the dataset
Drops the target column (Hypertension) before training
Splits & scales the data using StandardScaler
Trains an XGBoost classifier
Takes a row index (df.iloc[index]) to select a patient
Predicts "Yes" or "No" for hypertension
"""

import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Check if 'Hypertension' exists in df
if 'Hypertension' not in df.columns:
    print("Error: 'Hypertension' column not found in DataFrame. Available columns:", df.columns)
else:
    # Define features and target
    X = df.drop(columns=['Hypertension'])
    y = df['Hypertension']  # Define target

    # Split dataset
    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)

    # Scale features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_valid_scaled = scaler.transform(X_valid)

    # Initialize and train the XGBoost model
    model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
    model.fit(X_train_scaled, y_train)

    print("Model training complete!")

import xgboost as xgb
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split


# Function to predict hypertension for a specific patient (by index)

def predict_hypertension_by_index(index):
    if index >= len(df):
        print(f" Error: Index {index} is out of range!")
        return

    # Select the patient's data (row at the given index)
    patient_data = df.drop(columns=['Disease Severity']).iloc[index].values.reshape(1, -1)

    # Scale the patient's data
    patient_scaled = scaler.transform(patient_data)

    # Predict hypertension status
    prediction = model.predict(patient_scaled)[0]
    hypertension_status = "Yes" if prediction == 1 else "No"

    print(f" Predicted Hypertension for Patient at Index{index}: {hypertension_status} ")


# ðŸ”¹ Example Usage: Predict for patient at index 10
predict_hypertension_by_index (10)

"""Trained an XGBoost classifier on the dataset to predict whether a patient has hypertension (Yes or No). first preprocessed the data by scaling features using StandardScaler and then trained the model on the Hypertension column as the target variable.


For prediction, we selected a subset of patients (e.g., first 50) and used the trained model to predict their hypertension probability. If the probability is above 0.5, the model classifies the patient as "Yes" (Hypertension present); otherwise, it predicts "No" (No Hypertension).

"""

# Define features and target variable
X = df.drop(columns=['Disease Severity'])  # Drop the target column
y = df['Disease Severity']


# Predict Hypertension for Multiple Patients

def plot_hypertension_for_patients(num_patients=50):
    # Select the first 'num_patients' rows
    patient_data = df.drop(columns=['Disease Severity']).iloc[:num_patients]

    # Scale the patient data
    patient_scaled = scaler.transform(patient_data)

    # Get probabilities for hypertension (1 = High, 0 = Low)
    predictions = model.predict_proba(patient_scaled)[:, 1]  # Probability of hypertension (class 1)

    # Create a dataframe for plotting
    patient_ids = range(1, num_patients + 1)
    result_df = pd.DataFrame({'Patient': patient_ids, 'Hypertension Probability': predictions})

    # Sort patients by highest hypertension risk
    result_df = result_df.sort_values(by='Hypertension Probability', ascending=False)

    # ðŸ”¹ Plot
    plt.figure(figsize=(12, 6))
    sns.barplot(x='Patient', y='Hypertension Probability', data=result_df, palette="coolwarm")
    plt.xticks(rotation=90)
    plt.xlabel("Patient Index")
    plt.ylabel("Hypertension Probability")
    plt.title("Hypertension Risk Prediction for Patients")
    plt.show()

# Run function to visualize hypertension risks
plot_hypertension_for_patients(35)

"""The graph visualizes these predictions by plotting patient IDs vs. hypertension probability. The higher the bar, the greater the risk of hypertension. Patients are sorted from highest to lowest risk, making it easier to identify those who might need medical attention first. This method helps in risk assessment and prioritizing healthcare decisions.


Darker (red) bars = higher probability of hypertension

Lighter (blue) bars = lower probability of hypertension






"""



"""."""